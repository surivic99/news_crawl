{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class MarkdownChunker:\n",
    "    \"\"\"\n",
    "    Chunk Markdown text into pieces that fit within a token budget using a tokenizer.\n",
    "    Strategy:\n",
    "      - Split by Markdown headers first to preserve structure.\n",
    "      - For oversized sections, split hierarchically: paragraphs -> lines -> sentences -> words.\n",
    "      - Greedily pack adjacent units at each level while respecting max_tokens.\n",
    "      - As a last resort (e.g., a single long word), fall back to character-level greedy splitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: Optional[AutoTokenizer] = None, tokenizer_name: str = \"gpt2\"):\n",
    "        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        if self.tokenizer.pad_token is None and getattr(self.tokenizer, \"eos_token\", None):\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Precompile patterns\n",
    "        self.header_re = re.compile(r\"^(#{1,6}\\s+.*)$\", re.MULTILINE)\n",
    "\n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        if not text:\n",
    "            return 0\n",
    "        return len(self.tokenizer.encode(text, add_special_tokens=False))\n",
    "\n",
    "    def chunk(self, markdown_text: str, max_tokens: int) -> List[str]:\n",
    "        if not markdown_text or markdown_text.isspace():\n",
    "            return []\n",
    "\n",
    "        sections = self._split_by_headers(markdown_text)\n",
    "\n",
    "        # Split oversized sections first to ensure everything fits\n",
    "        normalized_sections: List[str] = []\n",
    "        for sec in sections:\n",
    "            normalized_sections.extend(self._split_section_to_fit(sec, max_tokens))\n",
    "\n",
    "        # Greedy merge adjacent sections while respecting max_tokens\n",
    "        return self._merge_sections(normalized_sections, max_tokens)\n",
    "\n",
    "    # --- Internal helpers ---\n",
    "\n",
    "    def _split_by_headers(self, text: str) -> List[str]:\n",
    "        parts = self.header_re.split(text)\n",
    "        sections: List[str] = []\n",
    "        preface = parts[0].strip() if parts and parts[0] else \"\"\n",
    "\n",
    "        if preface:\n",
    "            sections.append(preface)\n",
    "\n",
    "        for i in range(1, len(parts), 2):\n",
    "            header = parts[i].strip() if parts[i] else \"\"\n",
    "            content = (parts[i + 1] or \"\").strip() if i + 1 < len(parts) else \"\"\n",
    "            if header and content:\n",
    "                sections.append(f\"{header}\\n{content}\")\n",
    "            elif header:\n",
    "                sections.append(header)\n",
    "            elif content:\n",
    "                sections.append(content)\n",
    "\n",
    "        if not sections and parts and parts[0]:\n",
    "            sections.append(parts[0].strip())\n",
    "\n",
    "        return [s for s in sections if s.strip()]\n",
    "\n",
    "    def _merge_sections(self, sections: List[str], max_tokens: int) -> List[str]:\n",
    "        chunks: List[str] = []\n",
    "        current = \"\"\n",
    "\n",
    "        for sec in sections:\n",
    "            tentative = f\"{current}\\n\\n{sec}\" if current else sec\n",
    "            if self.count_tokens(tentative) <= max_tokens:\n",
    "                current = tentative\n",
    "            else:\n",
    "                if current:\n",
    "                    chunks.append(current.strip())\n",
    "                current = sec\n",
    "\n",
    "        if current:\n",
    "            chunks.append(current.strip())\n",
    "        return chunks\n",
    "\n",
    "    def _split_section_to_fit(self, section: str, max_tokens: int) -> List[str]:\n",
    "        if self.count_tokens(section) <= max_tokens:\n",
    "            return [section]\n",
    "\n",
    "        header, content = self._extract_header(section)\n",
    "\n",
    "        if header:\n",
    "            # Split content first\n",
    "            content_chunks = self._split_text_hierarchically(content, max_tokens)\n",
    "\n",
    "            # Try to attach header to the first content chunk\n",
    "            if content_chunks:\n",
    "                first_with_header = f\"{header}\\n{content_chunks[0]}\"\n",
    "                if self.count_tokens(first_with_header) <= max_tokens:\n",
    "                    out = [first_with_header]\n",
    "                    out.extend(content_chunks[1:])\n",
    "                    return out\n",
    "\n",
    "            # If header alone fits, keep it as its own chunk\n",
    "            if self.count_tokens(header) <= max_tokens:\n",
    "                return [header] + content_chunks\n",
    "\n",
    "            # If header itself is too large, split the header too\n",
    "            header_chunks = self._split_text_hierarchically(header, max_tokens)\n",
    "            return header_chunks + content_chunks\n",
    "\n",
    "        # No header: split the whole section\n",
    "        return self._split_text_hierarchically(section, max_tokens)\n",
    "\n",
    "    def _extract_header(self, section: str):\n",
    "        lines = section.splitlines()\n",
    "        if lines and re.match(r\"^#{1,6}\\s+.*$\", lines[0]):\n",
    "            header_line = lines[0]\n",
    "            rest = \"\\n\".join(lines[1:]).strip()\n",
    "            return header_line, rest\n",
    "        return None, section\n",
    "\n",
    "    def _split_text_hierarchically(self, text: str, max_tokens: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Attempts progressively finer splits and greedy packing at each level.\n",
    "        Always returns a non-empty list if text is non-empty; if a single atomic unit\n",
    "        still exceeds the budget, it will be emitted alone, or broken by characters as a last resort.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        # Stop if already fits\n",
    "        if self.count_tokens(text) <= max_tokens:\n",
    "            return [text]\n",
    "\n",
    "        # Levels: paragraphs -> lines -> sentences -> words\n",
    "        levels = [\n",
    "            (\"paragraphs\", self._split_paragraphs, \"\\n\\n\"),\n",
    "            (\"lines\", self._split_lines, \"\\n\"),\n",
    "            (\"sentences\", self._split_sentences, \" \"),\n",
    "            (\"words\", self._split_words, \" \"),\n",
    "        ]\n",
    "\n",
    "        units = [text]\n",
    "        joiner_for_level = \"\"\n",
    "\n",
    "        for _, splitter, joiner in levels:\n",
    "            # Expand any unit that is too large at current level\n",
    "            next_units: List[str] = []\n",
    "            any_split = False\n",
    "            for u in units:\n",
    "                if self.count_tokens(u) <= max_tokens:\n",
    "                    next_units.append(u)\n",
    "                    continue\n",
    "                parts = splitter(u)\n",
    "                if len(parts) == 1:\n",
    "                    next_units.append(u)\n",
    "                else:\n",
    "                    any_split = True\n",
    "                    next_units.extend([p for p in parts if p.strip() != \"\"])\n",
    "            units = next_units\n",
    "            joiner_for_level = joiner\n",
    "\n",
    "            # After splitting at this level, try to pack\n",
    "            packed = self._pack_units(units, joiner_for_level, max_tokens)\n",
    "            if packed is not None:\n",
    "                return packed\n",
    "\n",
    "            # If we couldn't split anything at this level, try the next\n",
    "            if not any_split:\n",
    "                continue\n",
    "            return self._pack_units(units, joiner_for_level, max_tokens) or [text]\n",
    "\n",
    "\n",
    "    def _pack_units(self, units: List[str], joiner: str, max_tokens: int) -> Optional[List[str]]:\n",
    "        \"\"\"\n",
    "        Greedily joins adjacent units with the given joiner while staying under max_tokens.\n",
    "        Returns None if any individual unit already exceeds max_tokens (needs deeper split).\n",
    "        \"\"\"\n",
    "        chunks: List[str] = []\n",
    "        current = \"\"\n",
    "\n",
    "        for unit in units:\n",
    "            if self.count_tokens(unit) > max_tokens:\n",
    "                return None  # needs deeper split\n",
    "            tentative = f\"{current}{joiner}{unit}\" if current else unit\n",
    "            if self.count_tokens(tentative) <= max_tokens:\n",
    "                current = tentative\n",
    "            else:\n",
    "                if current:\n",
    "                    chunks.append(current.strip())\n",
    "                current = unit\n",
    "\n",
    "        if current:\n",
    "            chunks.append(current.strip())\n",
    "        return chunks\n",
    "\n",
    "    # Splitters\n",
    "\n",
    "    def _split_paragraphs(self, text: str) -> List[str]:\n",
    "        return [p.strip() for p in re.split(r\"\\n{2,}\", text) if p.strip() != \"\"]\n",
    "\n",
    "    def _split_lines(self, text: str) -> List[str]:\n",
    "        return [ln for ln in text.split(\"\\n\") if ln.strip() != \"\"]\n",
    "\n",
    "    def _split_sentences(self, text: str) -> List[str]:\n",
    "        # Naive sentence splitter that keeps punctuation\n",
    "        parts = re.findall(r'.*?(?:[\\.!\\?](?!\\w)|$)', text, flags=re.S)\n",
    "        return [p.strip() for p in parts if p and p.strip()]\n",
    "\n",
    "    def _split_words(self, text: str) -> List[str]:\n",
    "        words = text.split()\n",
    "        return words if words else [text]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b47a6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "# Introduction\n",
      "This is an introductory paragraph that provides a general overview of the document. It should be grouped with the header above when possible, unless it's too large to fit in a chunk.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "## Section One\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
      "\n",
      "--- Chunk 3 ---\n",
      "### Subsection A\n",
      "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "\n",
      "#### Small Header\n",
      "Short line here.\n",
      "\n",
      "--- Chunk 4 ---\n",
      "##### Even Smaller Header\n",
      "Another short line.\n",
      "\n",
      "###### Tiny Header\n",
      "Final small header.\n",
      "\n",
      "## Section Two\n",
      "This section has a long list:\n",
      "- Item one\n",
      "- Item two\n",
      "- Item three\n",
      "- Item four\n",
      "- Item five\n",
      "- Item six\n",
      "- Item seven\n",
      "- Item eight\n",
      "- Item nine\n",
      "- Item ten\n",
      "\n",
      "And now we have a numbered list:\n",
      "1. First item\n",
      "2. Second item\n",
      "3. Third item\n",
      "4. Fourth item\n",
      "5. Fifth item\n",
      "\n",
      "--- Chunk 5 ---\n",
      "## Code Example\n",
      "Here’s some code:\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, world!\")\n",
      "    for i in range(10000):\n",
      "        print(f\"Number: {i}\")\n",
      "```\n",
      "\n",
      "## Blockquote\n",
      "> This is a blockquote.  \n",
      "> It spans multiple lines.  \n",
      "> Let's make sure it chunks correctly.\n",
      "\n",
      "--- Chunk 6 ---\n",
      "## Very Long Line\n",
      "This is a very long sentence that just keeps going on and on and on without any natural break points, which might force the chunker to split at the token level if nothing else works. We want to ensure that even such awkward content gets handled gracefully by the chunking logic.\n",
      "\n",
      "## Final Section\n",
      "Just a final wrap-up section to close things off. This paragraph may be short or combined with others depending on the max token limit used during chunking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = \"\"\"# Introduction\n",
    "\n",
    "This is an introductory paragraph that provides a general overview of the document. It should be grouped with the header above when possible, unless it's too large to fit in a chunk.\n",
    "\n",
    "## Section One\n",
    "\n",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
    "\n",
    "### Subsection A\n",
    "\n",
    "Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
    "\n",
    "#### Small Header\n",
    "\n",
    "Short line here.\n",
    "\n",
    "##### Even Smaller Header\n",
    "\n",
    "Another short line.\n",
    "\n",
    "###### Tiny Header\n",
    "\n",
    "Final small header.\n",
    "\n",
    "## Section Two\n",
    "\n",
    "This section has a long list:\n",
    "- Item one\n",
    "- Item two\n",
    "- Item three\n",
    "- Item four\n",
    "- Item five\n",
    "- Item six\n",
    "- Item seven\n",
    "- Item eight\n",
    "- Item nine\n",
    "- Item ten\n",
    "\n",
    "And now we have a numbered list:\n",
    "1. First item\n",
    "2. Second item\n",
    "3. Third item\n",
    "4. Fourth item\n",
    "5. Fifth item\n",
    "\n",
    "## Code Example\n",
    "\n",
    "Here’s some code:\n",
    "\n",
    "```python\n",
    "def hello_world():\n",
    "    print(\"Hello, world!\")\n",
    "    for i in range(10000):\n",
    "        print(f\"Number: {i}\")\n",
    "```\n",
    "\n",
    "## Blockquote\n",
    "\n",
    "> This is a blockquote.  \n",
    "> It spans multiple lines.  \n",
    "> Let's make sure it chunks correctly.\n",
    "\n",
    "## Very Long Line\n",
    "\n",
    "This is a very long sentence that just keeps going on and on and on without any natural break points, which might force the chunker to split at the token level if nothing else works. We want to ensure that even such awkward content gets handled gracefully by the chunking logic.\n",
    "\n",
    "## Final Section\n",
    "\n",
    "Just a final wrap-up section to close things off. This paragraph may be short or combined with others depending on the max token limit used during chunking.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chunker = MarkdownChunker(tokenizer_name=\"mixedbread-ai/mxbai-embed-xsmall-v1\")\n",
    "\n",
    "chunks = chunker.chunk(sample, max_tokens=100)  # Try different values: 50, 75, 100, etc.\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i+1} ---\")\n",
    "    print(chunk)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
